{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    " \n",
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import sql\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    " \n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil import relativedelta\n",
    "import time\n",
    "from time import clock\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 256)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    " \n",
    "# for beautiful output, prototype-only!\n",
    "def show(self, n=50):\n",
    "    return self.limit(n).toPandas()\n",
    " \n",
    "pyspark.sql.dataframe.DataFrame.show = show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_MAJOR_VERSION'] = '2'\n",
    "os.environ['SPARK_HOME'] = '/usr/sdp/3.4.0.1-1/spark2/'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/python/virtualenv/jupyter/lib'\n",
    "sys.path.insert(0, '/usr/sdp/3.4.0.1-1/spark2/python/')\n",
    "sys.path.insert(0, '/usr/sdp/3.4.0.1-1/spark2/python/lib/py4j-0.10.7-src.zip')\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "   .builder\n",
    "   .master(\"yarn-client\")\n",
    "   .config(\"spark.local.dir\", \"sparktmp\")\n",
    "   .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "   .config('spark.shuffle.service.enabled', True)\n",
    "   .config(\"spark.dynamicAllocation.minExecutors\", 2)\n",
    "   .config(\"spark.dynamicAllocation.maxExecutors\", 15)\n",
    "   .config(\"spark.kryoserializer.buffer.max\", \"2040mb\")\n",
    "   .config(\"spark.hadoop.hive.exec.dynamic.partition\", True)\n",
    "   .config(\"spark.hadoop.hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    " \n",
    "   .config(\"spark.sql.hive.caseSensitiveInferenceMode\", \"NEVER_INFER\")\n",
    "   # .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "   .config(\"spark.driver.maxResultSize\", \"6g\")\n",
    "   .config(\"spark.driver.memory\", \"8g\")\n",
    "   .config(\"spark.yarn.driver.memoryOverhead\", \"6048mb\")\n",
    " \n",
    "   .config(\"spark.executor.memory\", \"12g\")\n",
    "   .config(\"spark.executor.cores\", 4)\n",
    "   .config(\"spark.executor.instances\", 12)\n",
    "   .config(\"spark.yarn.driver.memoryOverhead\", \"6048mb\")\n",
    "    \n",
    "    .enableHiveSupport()\n",
    "   .appName('template')\n",
    "   .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d612b72",
   "metadata": {},
   "source": [
    "# Search in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5984378",
   "metadata": {},
   "source": [
    "## Search database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_NAME_SHOULD_CONTAIN = 'prx'\n",
    "spark.sql('show databases')\\\n",
    "   .filter(F.col('databaseName').contains(BASE_NAME_SHOULD_CONTAIN)).show()\n",
    " \n",
    "spark.catalog.listTables('prx_sbof_2_selfservice_sbof')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075f656",
   "metadata": {},
   "source": [
    "## Search columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6795e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAME_SHOULD_CONTAIN = 'inn'\n",
    "scheme_nm = 'cib_custom_cib_p4d_pprb_ucpkb'\n",
    "for table in spark.catalog.listTables(scheme_nm):\n",
    "   flg = 0\n",
    "   for column in spark.catalog.listColumns(table.name,scheme_nm):\n",
    "       if COLUMN_NAME_SHOULD_CONTAIN in column.name.lower():\n",
    "           print('Found column {} in table {}'.format(column.name.lower(),table.name))\n",
    "            flg = 1\n",
    "   if (flg == 1):\n",
    "       print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e0acb",
   "metadata": {},
   "source": [
    "## Search for joinable fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.table(\"cib_custom_cib_p4d_pprb_ucpkb.party_equivalent_equivalent\").limit(5000)\n",
    "b = spark.table(\"cib_custom_cib_p4d_pprb_ucpkb.party_identificatation_inn\").limit(5000)\n",
    " \n",
    "for col_a in a.columns:\n",
    "   for col_b in b.columns:\n",
    "       # If the types are not equal:\n",
    "       if a.select(col_a).dtypes[0][1] != b.select(col_b).dtypes[0][1]:\n",
    "           continue\n",
    "       else:\n",
    "           join_result = (\n",
    "           a.alias('a')\n",
    "            .join(b.alias('b'),\n",
    "                  on=(F.col('a.' + col_a) == F.col('b.' + col_b)))\n",
    "            .count())\n",
    "           if join_result:\n",
    "               print(\"type:\", a.select(col_a).dtypes[0][1],\n",
    "                     \"\\ncolumns:\", col_a, col_b, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43629cf4",
   "metadata": {},
   "source": [
    "# Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead21b77",
   "metadata": {},
   "source": [
    "## By which field a table is partitioned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426614d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_get_partition_names('sbx_t_team_ds_kb_sme.03_mart_antifraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035a64a",
   "metadata": {},
   "source": [
    "## Plot partition sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dea0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_partition_skew(df: DataFrame): # Рисует график распределения даных по партициям\n",
    "    (df.withColumn('partition_id', F.spark_partition_id())\n",
    "   .groupBy('partition_id').count()\n",
    "   .toPandas().sort_values('partition_id')\n",
    "              .plot(x='partition_id',\n",
    "                    y='count',\n",
    "                    kind='line'))\n",
    " \n",
    "show_partition_skew(spark.table('sbx_t_team_ds_kb_sme.03_mart_antifraud'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa35b2",
   "metadata": {},
   "source": [
    "## Generate multiple \"SHOW CREATE TABLE\"s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'cib_internal_eks_ibs'\n",
    "source_tables = ['z_ac_fin', 'z_depart', 'z_branch', 'z_ft_money', 'z_ps', 'z_intend_for',\n",
    "         'z_acc_product', 'z_type_acc', 'z_user']\n",
    " \n",
    "with open('Accounts_show_create_table_for_sources.txt', 'w') as f:\n",
    "   for table in source_tables:\n",
    "       raw_text = (spark.sql('show create table {}.{}'.format(schema, table))\n",
    "                   .show().iloc[0, 0])\n",
    "       f.write(\"======================================\\n\"\n",
    "               + \"===========\\t\" + table + \"\\t=============\\n\"\n",
    "               + \"======================================\\n\"\n",
    "               + raw_text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371fd2a",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ced6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(..., F.to_timestamp('effectivefrom', 'dd.MM.yyyy HH:mm:ss').alias('create'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a51517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('len', F.length('egrul_org_id'))\\\n",
    "   .groupby('len')\\\n",
    "   .agg(F.count('len').alias('count'))\\\n",
    "   .sort(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1865cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('ul_adrs_house_num',\n",
    "                 F.regexp_replace('ul_adrs_house_num', r'^[а-я]*\\.?', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301bca0",
   "metadata": {},
   "source": [
    "## 180 days window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df995a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "from calendar import monthrange\n",
    "date_to = (datetime.now() - timedelta(days = 1)).strftime(\"%Y-%m-%d\")\n",
    "date_from = (datetime.now() - timedelta(days = 180)).strftime(\"%Y-%m-%d\")\n",
    " \n",
    "window_over = Window.partitionBy('agr_cred_id').orderBy('gregor_dt').rowsBetween(179, Window.currentRow)\n",
    "spark.catalog.refreshTable(\"prx_igntv1_dwh_slcl.v_agr_cred\")\n",
    "agr_cred = spark\\\n",
    "   .table(\"prx_igntv1_dwh_slcl.v_agr_cred\")\\\n",
    "   .selectExpr(\n",
    "               \"host_agr_cred_id as contract_id\" ,\n",
    "               \"gregor_dt_part as report_dt\",\n",
    "               'gregor_dt',\n",
    "               'restruct_flag',\n",
    "               'last_restruct_dt as rest_dt',\n",
    "               'agr_cred_id',\n",
    "               'agr_num',\n",
    "               'cust_id',\n",
    "               'debt_ovr_dt',\n",
    "               'intrst_ovr_dt',\n",
    "               'expiration_dt as contract_close_dt',\n",
    "               'ovr_days_cnt'\n",
    "          )\\\n",
    "   .filter(F.col('report_dt').between(date_from, date_to))\\\n",
    "   .filter(F.col('contract_id') != \"-1\")\\\n",
    "   .withColumn('ovr_days_180',\n",
    "                F.sum(F.when(F.col('ovr_days_cnt') > 0, 1).otherwise(0)).over(window_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25c0ad",
   "metadata": {},
   "source": [
    "## Read Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1943008",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(pd.read_excel('fraud.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cb586",
   "metadata": {},
   "source": [
    "## Distinct by field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select('ul_status_nm').distinct().collect()\n",
    "# same in list:\n",
    "[i.ul_status_nm for i in result.select('ul_status_nm').distinct().collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af116b",
   "metadata": {},
   "source": [
    "## Count nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(F.isnan('request_num')).count()\n",
    " \n",
    "spark.table('prx_sbof_3_selfservice_sbof.dd_calendar')\\\n",
    "   .filter(F.col('pymondaystart').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc640129",
   "metadata": {},
   "source": [
    "### By all fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326b2cf",
   "metadata": {},
   "source": [
    "### In percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386940e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\\\n",
    ".T.rename(columns={0: '% Nulls'}).div(df.count()).mul(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d387c",
   "metadata": {},
   "source": [
    "## Count non-null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.na.drop().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341485c",
   "metadata": {},
   "source": [
    "## Delete columns which ends with _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fd56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.drop(*[col for col in result.columns if col.endswith('_x')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8df900",
   "metadata": {},
   "source": [
    "## Cell value in list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48647c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.filter(result.org_segment.isin('Микро', 'Малые', 'Малый')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d277a1a",
   "metadata": {},
   "source": [
    "## .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.groupby('org_segment')\\\n",
    "   .count().alias('amount').show()\n",
    " \n",
    "temp_df.groupBy('ul_head_inn').count()\\\n",
    "   .orderBy(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0987643",
   "metadata": {},
   "source": [
    "## Compare x strings by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efabc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    " \n",
    "def show_diff_in_rows(df: DataFrame, sort_by: str) -> pd.DataFrame:\n",
    "    \"\"\"Compare strings by columns. For example, if you want to comare all column values \n",
    "    in two rows respectively - you should filter the df to only those two rows left and pass it\n",
    "    as the first argument\n",
    "    \n",
    "    :param df: spark DataFrame (< 7 rows!!!)\n",
    "   :param sort_by: by which column we enumerate rows\n",
    "   :return: Pandas DataFrame with distinctive values\"\"\"\n",
    "    \n",
    "    def to_long(row: DataFrame, by: str) -> DataFrame:\n",
    "       \"\"\"Transpone the dataframe\n",
    "       :param row: spark Dataframe with one row\n",
    "        :param by: Name of the column which contains row number\n",
    "       :return: dataframe (row_number | Column_name | Respective_value1 | Respective_value2 | ...)\"\"\"\n",
    "        cols, dtypes = zip(*((c, t) for (c, t) in row.dtypes if c not in by))\n",
    "       assert len(set(dtypes)) == 1, \"All columns have to be the same type\"\n",
    " \n",
    "       kvs = F.explode(F.array([\n",
    "           F.struct(F.lit(c).alias(\"col_name\"), F.col(c).alias('val')) for c in cols\n",
    "       ])).alias('kvs')\n",
    "       return row.select(by + [kvs]).select(by + [\"kvs.col_name\", \"kvs.val\"])\n",
    " \n",
    "   df = df.withColumn('r_num', F.row_number().over(Window.orderBy(sort_by)))\n",
    "    # Conver everithing to String, so to_long() could do the job\n",
    "    df = df.select([F.col(c).cast(\"string\") for c in df.columns])\n",
    "   df_lst = [to_long(df.filter(F.col(\"r_num\") == i), [\"r_num\"])\n",
    "                 .withColumnRenamed('val', ('string_' + str(i)))\n",
    "             for i in range(1, df.count() + 1)]\n",
    "    \n",
    "    # Create the df with one column, which contains all column names of the input df\n",
    "    accum = spark.createDataFrame(data=[(i[0], ) for i in df_lst[0].select('col_name').collect()],\n",
    "                                  schema=[('col_name'),])\n",
    "   for elem in df_lst:\n",
    "       accum = accum.join(elem.drop(\"r_num\"), on=\"col_name\", how=\"inner\")\n",
    "        \n",
    "    for idx, elem in enumerate(accum):\n",
    "       accum = accum.filter(F.col(\"string_\" + str(idx+1)) != F.col(\"string_\" + str(idx + 2)))\n",
    "       if (idx == df.count() - 2): break\n",
    " \n",
    "   return accum.show()\n",
    " \n",
    "# Example usage:    \n",
    "show_diff_in_rows(spark.table('cib_custom_cb_akm_integrum.ul_organization_egrul')\n",
    "                         .filter(F.col('ul_inn') == '3703023078'),\n",
    "                'effectiveFrom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5c3da",
   "metadata": {},
   "source": [
    "# Saving and deleting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e3417",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"sbx_t_team_ds_kb_sme.companies_by_ul_head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bf19c",
   "metadata": {},
   "source": [
    "## Resave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.table('cib_custom_cib_ml360.u_client_products')\n",
    ".write.format('parquet')\n",
    ".mode('overwrite')\n",
    ".saveAsTable('sbx_t_team_ds_kb_sme.u_client_products'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9a9d0",
   "metadata": {},
   "source": [
    "## With partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc830be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.table('sbx_t_team_ds_kb_sme.partner_cred')\n",
    ".write.format('parquet')\n",
    ".mode('overwrite').partitionBy(\"part_dt\")\n",
    ".saveAsTable('sbx_t_team_ds_kb_sme.partner_cred_backup'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba9ac4",
   "metadata": {},
   "source": [
    "## Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0100df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"truncate table sbx_t_team_ds_kb_sme.table\") # removes all the rows\n",
    "hive_drop_table('sbx_t_team_ds_kb_sme.table') # delete metainfo\n",
    "spark.sql(\"drop table sbx_t_team_ds_kb_sme.table purge\") # delete everithing, including parquet files!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948fbe4b",
   "metadata": {},
   "source": [
    "# Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version, version_info\n",
    " \n",
    "print(version)\n",
    "print(version_info)\n",
    " \n",
    "!which python3\n",
    " \n",
    "from sys import version, path\n",
    " \n",
    "print(version)\n",
    "import defusedxml\n",
    "print('defusedxml', defusedxml._version_)\n",
    "import setuptools\n",
    "print('setuptools', setuptools._version_)\n",
    "import exchangelib\n",
    "print('exchangelib', exchangelib._version_)\n",
    "import pip\n",
    "print('pip', pip._version_)\n",
    "import ssl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
